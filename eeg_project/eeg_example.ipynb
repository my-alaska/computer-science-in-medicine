{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HMS - harmful brain activity\n",
    "\n",
    "The aim of this project is to create and train a deep learning model that will detect harmful brain activity from EEG signal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries we're going to use\n",
    "\n",
    "`pandas` - a library for reading and processing data frames and input data files\n",
    "`numpy` - mathematical library for efficient multidimensional algebra and raw data processing\n",
    "`pytorch` - deep learning library and framework that allows for efficient data processing and neural network training using cpu or gpu\n",
    "`scikit-learn` - sklearn - Rich in functionality machine learning and data science library. Industry standard for ML\n",
    "`scipy` - scientific library including many mathematical tools. For example for signal processing\n",
    "`os` - operating system packet that allows for navigating in the file system from the level of python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:22.505910700Z",
     "start_time": "2024-05-17T13:44:21.059719900Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn\n",
    "import scipy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to fully utilise our resources. We use `cuda` - a tool used by `pytorch` to perform computation on gpu - this way neural networks can be trained much faster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:22.571909700Z",
     "start_time": "2024-05-17T13:44:22.495911200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "The data comes from [kaggle](https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification) competition. \n",
    "\n",
    " It consists of `.parquet` files with eeg signal data as well as plotted spectrograms of the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:22.583910600Z",
     "start_time": "2024-05-17T13:44:22.542910700Z"
    }
   },
   "outputs": [],
   "source": [
    "# directory that contains data from kaggle hms\n",
    "INPUT_DATA_DIR = \"data\"\n",
    "\n",
    "# directory in which our npy files are/will be stored - this will allow for faster loading of the data later\n",
    "PROCESSED_DATA_DIR = \"processed_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-09T14:46:01.191866Z",
     "iopub.status.busy": "2024-03-09T14:46:01.191481Z",
     "iopub.status.idle": "2024-03-09T14:46:01.205753Z",
     "shell.execute_reply": "2024-03-09T14:46:01.204294Z",
     "shell.execute_reply.started": "2024-03-09T14:46:01.191832Z"
    }
   },
   "source": [
    "The metadata is stored in train.csv and test.csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:22.636910400Z",
     "start_time": "2024-05-17T13:44:22.557911Z"
    }
   },
   "outputs": [],
   "source": [
    "train_meta_full = pd.read_csv(INPUT_DATA_DIR + \"/train.csv\")\n",
    "train_meta = train_meta_full.loc[train_meta_full[\"eeg_sub_id\"] == 0]\n",
    "\n",
    "test_meta = pd.read_csv(INPUT_DATA_DIR + \"/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Signal data tends to be very noisy. That's why it's important to filter the noise before processing the data further. For this we'll use the butterworth lowpass filter.\n",
    "\n",
    "Lowpass filter filters out high and noisy frequencies. It only allows the frequencies lower than specified to pass through it.\n",
    "\n",
    "The data is sampled at the frequency of 200Hz, we want to cut off noise frequencies higher than 20Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:22.657910800Z",
     "start_time": "2024-05-17T13:44:22.638910700Z"
    }
   },
   "outputs": [],
   "source": [
    "cutoff_freq = 20\n",
    "sampling_rate = 200\n",
    "order = 4\n",
    "b, a = scipy.signal.butter(\n",
    "    order, cutoff_freq, btype=\"low\", analog=False, fs=sampling_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Extract `.parquet` data.\n",
    "\n",
    "The training data is stored in `.parquet` files as individual read values of separate electrodes placed on patient's head during EEG examination.\n",
    "\n",
    "Normally specialists don't analyze the separate signals but rather differences between the neighbouring electrodes. \n",
    "\n",
    "`extract_parquet` function computes those differences and creates time series of those differences as opposed to pure signal.\n",
    "\n",
    "Those differences are then processed with previously initialized lowpass filter, clipped so that their values are not too high and converted to pytorch tensor objects.\n",
    "\n",
    "We want to use pytorch tensors because it's the object class required to perform neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:22.678910Z",
     "start_time": "2024-05-17T13:44:22.655910400Z"
    }
   },
   "outputs": [],
   "source": [
    "# take a parquet dataframe and compute correct values for each column\n",
    "# we want columns such as \"Fp1-F7\" as can be seen in /example_figures\n",
    "def extract_parquet(parquet_data: torch.tensor):\n",
    "    parquet_data[\"Fp1-F7\"] = parquet_data[\"Fp1\"] - parquet_data[\"F7\"]\n",
    "    parquet_data[\"F7-T3\"] = parquet_data[\"F7\"] - parquet_data[\"T3\"]\n",
    "    parquet_data[\"T3-T5\"] = parquet_data[\"T3\"] - parquet_data[\"T5\"]\n",
    "    parquet_data[\"T5-O1\"] = parquet_data[\"T5\"] - parquet_data[\"O1\"]\n",
    "\n",
    "    parquet_data[\"Fp2-F8\"] = parquet_data[\"Fp2\"] - parquet_data[\"F8\"]\n",
    "    parquet_data[\"F8-T4\"] = parquet_data[\"F8\"] - parquet_data[\"T4\"]\n",
    "    parquet_data[\"T4-T6\"] = parquet_data[\"T4\"] - parquet_data[\"T6\"]\n",
    "    parquet_data[\"T6-O2\"] = parquet_data[\"T6\"] - parquet_data[\"O2\"]\n",
    "\n",
    "    parquet_data[\"Fp1-F3\"] = parquet_data[\"Fp1\"] - parquet_data[\"F3\"]\n",
    "    parquet_data[\"F3-C3\"] = parquet_data[\"F3\"] - parquet_data[\"C3\"]\n",
    "    parquet_data[\"C3-P3\"] = parquet_data[\"C3\"] - parquet_data[\"P3\"]\n",
    "    parquet_data[\"P3-O1\"] = parquet_data[\"P3\"] - parquet_data[\"O1\"]\n",
    "\n",
    "    parquet_data[\"Fp2-F4\"] = parquet_data[\"Fp2\"] - parquet_data[\"F4\"]\n",
    "    parquet_data[\"F4-C4\"] = parquet_data[\"F4\"] - parquet_data[\"C4\"]\n",
    "    parquet_data[\"C4-P4\"] = parquet_data[\"C4\"] - parquet_data[\"P4\"]\n",
    "    parquet_data[\"P4-O2\"] = parquet_data[\"P4\"] - parquet_data[\"O2\"]\n",
    "\n",
    "    parquet_data[\"Fz-Cz\"] = parquet_data[\"Fz\"] - parquet_data[\"Cz\"]\n",
    "    parquet_data[\"Cz-Pz\"] = parquet_data[\"Cz\"] - parquet_data[\"Pz\"]\n",
    "\n",
    "    parquet_data = parquet_data.drop(\n",
    "        [\n",
    "            \"Fp1\",\n",
    "            \"F3\",\n",
    "            \"C3\",\n",
    "            \"P3\",\n",
    "            \"F7\",\n",
    "            \"T3\",\n",
    "            \"T5\",\n",
    "            \"O1\",\n",
    "            \"Fz\",\n",
    "            \"Cz\",\n",
    "            \"Pz\",\n",
    "            \"Fp2\",\n",
    "            \"F4\",\n",
    "            \"C4\",\n",
    "            \"P4\",\n",
    "            \"F8\",\n",
    "            \"T4\",\n",
    "            \"T6\",\n",
    "            \"O2\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # we want to reorder the columns so that the EKG signal is at the end.\n",
    "    idx = parquet_data.columns[1:].to_list() + [parquet_data.columns[0]]\n",
    "\n",
    "    # we want to transpose the values so that they're easier to handle later\n",
    "    parquet_data = parquet_data[idx].values.T\n",
    "\n",
    "    # filter the high frequencies\n",
    "    parquet_data = scipy.signal.lfilter(b, a, parquet_data, axis=0)\n",
    "\n",
    "    # convert to pytorch tensor\n",
    "    parquet_data = torch.from_numpy(parquet_data).type(torch.float32)\n",
    "\n",
    "    # clip high values to 1024\n",
    "    parquet_data = torch.clip(parquet_data, -1024, 1024)\n",
    "\n",
    "    return parquet_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the data from the `.parquet` files, so in a loop we'll iterate through the metadata and extract the data and the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:25.668539800Z",
     "start_time": "2024-05-17T13:44:22.668911100Z"
    }
   },
   "outputs": [],
   "source": [
    "# A list for data entries from different files\n",
    "eeg_data = []\n",
    "\n",
    "# indices of the dataframes with too many missing values\n",
    "faulty_eeg_id = []\n",
    "\n",
    "if not os.path.exists(f\"{PROCESSED_DATA_DIR}/eeg_labels.pt\"):\n",
    "\n",
    "    # iterate through IDs of eeg and extract each .parquet file\n",
    "    for eeg_id in train_meta[\"eeg_id\"]:\n",
    "\n",
    "        # open the file using pandas\n",
    "        parquet_data = pd.read_parquet(INPUT_DATA_DIR + f\"/train_eegs/{eeg_id}.parquet\")\n",
    "\n",
    "        # fill the missing values and extract the first 10000 measurements\n",
    "        parquet_data = parquet_data.interpolate(method=\"ffill\")[:10000]\n",
    "\n",
    "        # If at this point there are any missing values, the file can't be used\n",
    "        if np.any(parquet_data.isna()):\n",
    "            faulty_eeg_id.append(eeg_id)\n",
    "            continue\n",
    "\n",
    "        # we call the preprocessing function\n",
    "        eeg = extract_parquet(parquet_data)\n",
    "\n",
    "        # and add the data to the list of processed entries\n",
    "        eeg_data.append(eeg)\n",
    "\n",
    "    # convert the list of tensors to one tensor\n",
    "    eeg_data = torch.stack(eeg_data)\n",
    "\n",
    "    # save to PROCESSED_DATA_TIR\n",
    "    torch.save(eeg_data, f\"{PROCESSED_DATA_DIR}/eeg_data.pt\")\n",
    "\n",
    "    # extract labels for valid entries\n",
    "    all_labels = train_meta.loc[~train_meta[\"eeg_id\"].isin(faulty_eeg_id)]\n",
    "    all_labels = all_labels[\n",
    "        [\"seizure_vote\", \"lpd_vote\", \"gpd_vote\", \"lrda_vote\", \"grda_vote\", \"other_vote\"]\n",
    "    ].values\n",
    "\n",
    "    # filter out the entries with missing values\n",
    "    eeg_labels = all_labels[np.where(1 - train_meta[\"eeg_id\"].isin(faulty_eeg_id))]\n",
    "\n",
    "    # convert labels to pytorch tensor\n",
    "    eeg_labels = torch.tensor(np.array(eeg_labels), dtype=torch.float32)\n",
    "\n",
    "    # The labels are probability - they must sum to 1\n",
    "    eeg_labels = eeg_labels / eeg_labels.sum(dim=1, keepdims=True)\n",
    "\n",
    "    # save for easier training later\n",
    "    torch.save(eeg_labels, f\"{PROCESSED_DATA_DIR}/eeg_labels.pt\")\n",
    "else:\n",
    "    eeg_data = torch.load(f\"{PROCESSED_DATA_DIR}/eeg_data.pt\")\n",
    "    eeg_labels = torch.load(f\"{PROCESSED_DATA_DIR}/eeg_labels.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17018, 19, 10000])\n",
      "torch.Size([17018, 6])\n"
     ]
    }
   ],
   "source": [
    "print(eeg_data.shape)\n",
    "print(eeg_labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:25.689539100Z",
     "start_time": "2024-05-17T13:44:25.669539800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a data loader, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup global variables for dataloder and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:25.731540Z",
     "start_time": "2024-05-17T13:44:25.685540300Z"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLING_FREQUENCY = 200\n",
    "SAMPLES_IN_MEASUREMENT = 10000\n",
    "FOLDS = 5\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using scikit-learn we'll split the data into training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:26.512539600Z",
     "start_time": "2024-05-17T13:44:25.701540400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    eeg_data, eeg_labels, test_size=0.1, stratify=eeg_labels.argmax(axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform the computation on gpu, we have to move the data into cuda"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:28.943940200Z",
     "start_time": "2024-05-17T13:44:26.512539600Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = X_train.to(DEVICE), y_train.to(DEVICE)\n",
    "X_test, y_test = X_test.to(DEVICE), y_test.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a HMS dataset class that will help us load the data during the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:28.959940Z",
     "start_time": "2024-05-17T13:44:28.946941400Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    CustomImageDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    CustomImageDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model\n",
    "\n",
    "We create a machine learning model - a python object that will be trained to predict correct data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:28.981940500Z",
     "start_time": "2024-05-17T13:44:28.962940100Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleEEGModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super(SimpleEEGModel, self).__init__()\n",
    "\n",
    "        # LSTM is a recursive neural network unit that works well with sequences\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=19, hidden_size=50, num_layers=2, batch_first=True, dropout=0.0\n",
    "        )\n",
    "\n",
    "        # linear model - fully connected layer\n",
    "        self.clf = nn.Linear(in_features=50, out_features=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # we transpose the input and then pass it through LSTM\n",
    "        x, _ = self.lstm(x.permute(0, 2, 1))\n",
    "\n",
    "        # We're interested in the last lstm output\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # the LSTM output is passed through a fully connected layer\n",
    "        x = self.clf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        # additionally to the previous approach we use convolution.\n",
    "        # It downsizes the input from shape (19, 10000) to (32, 2500)\n",
    "        self.conv = nn.Conv1d(19, 32, kernel_size=4, stride=4)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=32, hidden_size=50, num_layers=2, batch_first=True, dropout=0.0\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(50, 6)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    # input in CHW / CW format\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, seq_length, input_size)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        out = self.softmax(x)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:28.998940300Z",
     "start_time": "2024-05-17T13:44:28.979940500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "After data preprocessing and buikding the model we need to train it. \n",
    "\n",
    "We'll use a [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) as a loss function. \n",
    "KLDivergence is the widely used metric suggested by the scientists who worked on the best solutions for the HMS competition.\n",
    "\n",
    "We also use Adam optimizier - an adaptive learning rate optimization algorithm used in training deep learning models.\n",
    "\n",
    "It combines the advantages of two other extensions of stochastic gradient descent \n",
    "- (SGD): Adaptive Gradient Algorithm (AdaGrad) \n",
    "- Root Mean Square Propagation (RMSProp)\n",
    "\n",
    "It adjusts the learning rate for each parameter dynamically, making it efficient and well-suited for large datasets and complex models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:44:30.872146Z",
     "start_time": "2024-05-17T13:44:28.992940200Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# initialize the model and move it to gpu\n",
    "model = ConvLSTM().to(DEVICE)\n",
    "\n",
    "# Create an optimizer that will perform the gradient descent algorithm\n",
    "optimizer = Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# As a loss function we'll be using KLDivergence\n",
    "loss_function = nn.KLDivLoss(reduction=\"batchmean\", log_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:45:36.727843700Z",
     "start_time": "2024-05-17T13:44:30.872146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "1.324487676451251\n",
      "==========================\n",
      "==========================\n",
      "1.3232252441020003\n",
      "==========================\n",
      "==========================\n",
      "1.3230708882305966\n",
      "==========================\n",
      "==========================\n",
      "1.3231792589319027\n",
      "==========================\n",
      "==========================\n",
      "1.3229767992501469\n",
      "==========================\n",
      "==========================\n",
      "1.3231270037015943\n",
      "==========================\n",
      "==========================\n",
      "1.3230768327673192\n",
      "==========================\n",
      "==========================\n",
      "1.323077905153183\n",
      "==========================\n",
      "==========================\n",
      "1.3231064524182696\n",
      "==========================\n",
      "==========================\n",
      "1.3231619837886357\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "# we train the network for EPOCHS epochs\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # variables that will let us compute average epoch loss\n",
    "    iteration = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    # iterate through data batches in the data loader\n",
    "    for batch in train_loader:\n",
    "\n",
    "        # extract data and labels\n",
    "        batch_data, batch_labels = batch\n",
    "\n",
    "        # perform the inference on the data\n",
    "        prediction = model(batch_data)\n",
    "\n",
    "        # compare the inference output with real labels and calculate the value of loss function\n",
    "        loss = loss_function(nn.functional.log_softmax(prediction, dim=1), batch_labels)\n",
    "        # print(loss.item())\n",
    "\n",
    "        # accumulate loss to print the average at the end of the epoch\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "        # reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute the error backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights of the model\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    # scheduler.step()\n",
    "    print(\"==========================\")\n",
    "    print(total_loss / iteration)\n",
    "    print(\"==========================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T16:13:47.666559Z",
     "iopub.status.busy": "2024-03-10T16:13:47.666106Z",
     "iopub.status.idle": "2024-03-10T16:13:47.673670Z",
     "shell.execute_reply": "2024-03-10T16:13:47.672143Z",
     "shell.execute_reply.started": "2024-03-10T16:13:47.666526Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3376106730213873\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    iteration = 0\n",
    "    total_loss = 0\n",
    "    for batch in test_loader:\n",
    "\n",
    "        # extract data and labels\n",
    "        batch_data, batch_labels = batch\n",
    "\n",
    "        # perform the inference on the data\n",
    "        prediction = model(batch_data)\n",
    "\n",
    "        # compare the inference output with real labels and calculate the value of loss function\n",
    "        loss = loss_function(nn.functional.log_softmax(prediction, dim=1), batch_labels)\n",
    "\n",
    "        # accumulate loss\n",
    "        total_loss += float(loss.item())\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "print(total_loss / iteration)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T13:45:37.068461400Z",
     "start_time": "2024-05-17T13:45:36.727697600Z"
    }
   }
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 4317718,
     "sourceId": 7465251,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 166350260,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
